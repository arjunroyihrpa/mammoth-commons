
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Analysis metrics</title>
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">
    <link rel="stylesheet" href="{{ url_for('static', filename='styles.css') }}">
    <style>
        .main-content {
            margin-top: 60px;
        }
        .side-menu {
            height: 100vh;
            position: fixed;
            top: 0;
            left: 0;
            width: 200px;
            padding-top: 70px;
            background-color: #f8f9fa;
            border-right: 1px solid #dee2e6;
        }
        .side-menu a {
            display: block;
            padding: 5px 20px;
            color: #495057;
            text-decoration: none;
        }
        .side-menu a:hover {
            background-color: #e9ecef;
        }
        .content-wrapper {
            margin-left: 220px;
            margin-right: 220px;
            padding: 20px;
        }
        h2 {
            color: #007bff;
            border-bottom: 3px solid #007bff;
            padding-bottom: 5px;
            margin-top: 20px;
        }
        
        /* Hide navbar on small screens */
        @media (max-width: 576px) {
            .side-menu {
                display: none;
            }
    
            .content-wrapper {
                margin-left: 0;
                margin-right: 0;
                padding: 0;
            }
            .main-content {
                margin-top: 200px;
            }
        }
    </style>
</head>
<body>
    <!-- Top Navbar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="mx-auto">
            <ul class="navbar-nav">
                <li class="nav-item">
                    <a class="nav-link" href="index.html">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="datasets.html">Datasets</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="models.html">Models</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="analysis_methods.html">Analysis</a>
                </li>
            </ul>
        </div>
    </nav>

    <!-- Side Menu -->
    <div class="side-menu">
        <a href='#card'>Card</a>
<a href='#interactive-report'>Interactive report</a>
<a href='#interactive-sklearn-report'>Interactive sklearn report</a>
<a href='#image-bias-analysis'>Image bias analysis</a>
<a href='#facex-regions'>Facex regions</a>
<a href='#facex-embeddings'>Facex embeddings</a>
<a href='#connection-properties'>Connection properties</a>
<a href='#exposure-distance-comparison'>Exposure distance comparison</a>

    </div>

    <!-- Main Content -->
    <div class="content-wrapper">
        <div class="main-content container">
            <div class="container">
<h1 class='display-4 text-center my-4'>Analysis metrics</h1>
<h2 id='card'>Card</h2>
<p><p>Creates a model card using the <a href="https://github.com/mever-team/FairBench">FairBench</a> library. The card includes several fairness stamps; these are specific measures of bias or fairness that are commonly used in the algorithmic fairness literature. Only the most prominent of those measures are used as stamps, and they correspond to a perfunctory fairness analysis.</p>  <p>This module computes all applicable FairBench stamps, which summarize behavior across all population groups or intersectional subgroups. Multiple sensitive attributes may be present, such as gender, age, and race. Furthermore, each of those attributes may obtain multiple values, as happens when multiple genders or races are considered. Numeric attributes, like age, are normalized to the range [0,1] and we consider the result as truth values of membership to the group of the maximum value - as opposed to membership to the group with minimum value. A different stamp is computed for each prediction label.</p>  <p>You may optionally create intersectional subgroups, that is, create a separate subgroup for each combination of sensitive attribute values. Many of those groups will have few members if there are too many attributes, and empty groups are ignored during the analysis.</p>  <p>The created model card contains exact descriptions of methods used to compute fairness under the selected stamps, and it lists population groups that were taken into account These come alongside an extensive list of caveats and recommendations that help the reader get a grasp on how they should account for the social context. This material is retrieved from FairBench's online socio-technical database generated through MAMMOth's multidisciplinary activities.</p>  <p>Finally, the generated model card may contain details about out-of-the-box datasets. To get the full picture, a detailed fairness report that also allows you to backtrack computations is available in the <code>interactive report</code> module.</p> </p>
<b>Parameters</b><br> <button
                      type="button"
                      class="btn btn-light"
                      data-bs-toggle="tooltip"
                      data-bs-placement="top"
                      title=" Whether to consider all non-empty group intersections during analysis. This does nothing if there is only one sensitive attribute, but may also be computationally intensive if too many group intersections are selected."
                      data-description=" Whether to consider all non-empty group intersections during analysis. This does nothing if there is only one sensitive attribute, but may also be computationally intensive if too many group intersections are selected."
                      data-name="Intersectional"
                      onclick="showDescriptionModal(this)">
                      <i class="bi bi-info-circle"></i> Intersectional
                    </button>
<button
                      type="button"
                      class="btn btn-light"
                      data-bs-toggle="tooltip"
                      data-bs-placement="top"
                      title=" Whether to compare groups pairwise, or each group to the whole population. For example, if the 4/5ths rule stamp is applicable, it computes positive rates and obtains the minimum ratio, either across all pairs of groups (for pairwise comparison) or otherwise between each group and the total population."
                      data-description=" Whether to compare groups pairwise, or each group to the whole population. For example, if the 4/5ths rule stamp is applicable, it computes positive rates and obtains the minimum ratio, either across all pairs of groups (for pairwise comparison) or otherwise between each group and the total population."
                      data-name="Compare groups"
                      onclick="showDescriptionModal(this)">
                      <i class="bi bi-info-circle"></i> Compare groups
                    </button>
<br>
<h2 id='interactive-report'>Interactive report</h2>
<p><p>Creates an interactive report using the FairBench library. The report creates traceable evaluations that you can shift through to find actual sources of unfairness.</p> </p>
<b>Parameters</b><br> <button
                      type="button"
                      class="btn btn-light"
                      data-bs-toggle="tooltip"
                      data-bs-placement="top"
                      title=" Whether to consider all non-empty group intersections during analysis. This does nothing if there is only one sensitive attribute."
                      data-description=" Whether to consider all non-empty group intersections during analysis. This does nothing if there is only one sensitive attribute."
                      data-name="Intersectional"
                      onclick="showDescriptionModal(this)">
                      <i class="bi bi-info-circle"></i> Intersectional
                    </button>
<button
                      type="button"
                      class="btn btn-light"
                      data-bs-toggle="tooltip"
                      data-bs-placement="top"
                      title=" Whether to compare groups pairwise, or each group to the whole population."
                      data-description=" Whether to compare groups pairwise, or each group to the whole population."
                      data-name="Compare groups"
                      onclick="showDescriptionModal(this)">
                      <i class="bi bi-info-circle"></i> Compare groups
                    </button>
<br>
<h2 id='interactive-sklearn-report'>Interactive sklearn report</h2>
<p><p>Creates an interactive report using the FairBench library, after running an internal training-test split on a basic sklearn model. The report creates traceable evaluations that you can shift through to find sources of unfairness on a common task.</p> </p>
<b>Parameters</b><br> <button
                      type="button"
                      class="btn btn-light"
                      data-bs-toggle="tooltip"
                      data-bs-placement="top"
                      title=" Which sklearn predictor should be used."
                      data-description=" Which sklearn predictor should be used."
                      data-name="Predictor"
                      onclick="showDescriptionModal(this)">
                      <i class="bi bi-info-circle"></i> Predictor
                    </button>
<button
                      type="button"
                      class="btn btn-light"
                      data-bs-toggle="tooltip"
                      data-bs-placement="top"
                      title=" Whether to consider all non-empty group intersections during analysis. This does nothing if there is only one sensitive attribute."
                      data-description=" Whether to consider all non-empty group intersections during analysis. This does nothing if there is only one sensitive attribute."
                      data-name="Intersectional"
                      onclick="showDescriptionModal(this)">
                      <i class="bi bi-info-circle"></i> Intersectional
                    </button>
<button
                      type="button"
                      class="btn btn-light"
                      data-bs-toggle="tooltip"
                      data-bs-placement="top"
                      title=" Whether to compare groups pairwise, or each group to the whole population."
                      data-description=" Whether to compare groups pairwise, or each group to the whole population."
                      data-name="Compare groups"
                      onclick="showDescriptionModal(this)">
                      <i class="bi bi-info-circle"></i> Compare groups
                    </button>
<br>
<h2 id='image-bias-analysis'>Image bias analysis</h2>
<p><p>This module provides a comprehensive solution for analyzing image bias and recommending effective mitigation strategies. It can be used for both classification tasks (e.g., facial attribute extraction) and face verification. The core functionality revolves around evaluating how well different population groups, defined by a given protected attribute (such as gender, age, or ethnicity), are represented in the dataset. Representation bias occurs when some groups are overrepresented or underrepresented, leading to models that may perform poorly or unfairly on certain groups.</p>  <p>Additionally, the module detects spurious correlations between the target attribute (e.g., the label a model is trying to predict) and other annotated attributes (such as image features like color or shape). Spurious correlations are misleading patterns that do not reflect meaningful relationships and can cause a model to make biased or inaccurate predictions. By identifying and addressing these hidden biases, the module helps improve the fairness and accuracy of your model.</p>  <p>When you run the analysis, the module identifies specific biases within the dataset and suggests tailored mitigation approaches. Specifically, the suitable mitigation methodologies are determined based on the task and the types of the detected biases in the data. The analysis is conducted based on the <a href="https://github.com/gsarridis/cv-bias-mitigation-library">CV Bias Mitigation Library</a>.</p> </p>
<b>Parameters</b><br> <button
                      type="button"
                      class="btn btn-light"
                      data-bs-toggle="tooltip"
                      data-bs-placement="top"
                      title=" The type of predictive task. It should be either face verification or image classification."
                      data-description=" The type of predictive task. It should be either face verification or image classification."
                      data-name="Task"
                      onclick="showDescriptionModal(this)">
                      <i class="bi bi-info-circle"></i> Task
                    </button>
<br>
<h2 id='facex-regions'>Facex regions</h2>
<p><p><a href="https://github.com/gsarridis/faceX">FaceX</a> is a tool designed to help you understand how face attribute classifiers make decisions. It provides clear explanations by analyzing 19 key regions of the face, such as the eyes, nose, mouth, hair, and skin. This method helps reveal which parts of the face the model focuses on when making predictions about attributes like age, gender, or race.</p>  <p>Rather than explaining each individual image separately, FaceX aggregates information across the entire dataset, offering a broader view of the model's behavior. It looks at how the model activates different regions of the face for each decision. This aggregated information helps you see which facial features are most influential in the model's predictions, and whether certain features are being emphasized more than others.</p>  <p>In addition to providing an overall picture of which regions are important, FaceX also zooms in on specific areas of the face - such as a section of the skin or a part of the hair - showing which patches of the image have the highest impact on the model's decision. This makes it easier to identify potential biases or problems in how the model is interpreting the face.</p>  <p>Overall, with FaceX, you can quickly and easily get a better understanding of your model's decision-making process. This is especially useful for ensuring that your model is fair and transparent, and for spotting any potential biases that may affect its performance. <span class="alert alert-warning alert-dismissible fade show" role="alert" style="display: inline-block; padding: 10px;"> <i class="bi bi-exclamation-triangle-fill"></i> GPU access is recommended as this analysis can be computationally intensive, especially with large datasets. </span></p> </p>
<b>Parameters</b><br> <button
                      type="button"
                      class="btn btn-light"
                      data-bs-toggle="tooltip"
                      data-bs-placement="top"
                      title=" This parameter allows you to specify which class you want to analyze. For example, if you are using the model to classify faces by gender, setting the target class to male (i.e., the integer identifier for males class) will show you how the model makes decisions about male faces."
                      data-description=" This parameter allows you to specify which class you want to analyze. For example, if you are using the model to classify faces by gender, setting the target class to male (i.e., the integer identifier for males class) will show you how the model makes decisions about male faces."
                      data-name="Target class"
                      onclick="showDescriptionModal(this)">
                      <i class="bi bi-info-circle"></i> Target class
                    </button>
<button
                      type="button"
                      class="btn btn-light"
                      data-bs-toggle="tooltip"
                      data-bs-placement="top"
                      title=" This parameter lets you choose which part of the model's neural network you want to analyze. In simple terms, a model consists of multiple layers that process information at different levels. The target layer refers to the specific layer in the model that you want to explain. The explanation will show you which regions of the face are most important to that layer's decision-making process. For example, the deeper layers of the model may focus on more complex features like facial structure, while earlier layers might focus on simpler features like edges and textures. Typically, you should opt for the last layer prior to the classification layer."
                      data-description=" This parameter lets you choose which part of the model's neural network you want to analyze. In simple terms, a model consists of multiple layers that process information at different levels. The target layer refers to the specific layer in the model that you want to explain. The explanation will show you which regions of the face are most important to that layer's decision-making process. For example, the deeper layers of the model may focus on more complex features like facial structure, while earlier layers might focus on simpler features like edges and textures. Typically, you should opt for the last layer prior to the classification layer."
                      data-name="Target layer"
                      onclick="showDescriptionModal(this)">
                      <i class="bi bi-info-circle"></i> Target layer
                    </button>
<br>
<h2 id='facex-embeddings'>Facex embeddings</h2>
<p><p><a href="https://github.com/gsarridis/faceX">FaceX</a> for feature extractors is designed to help you understand how face verification models process images by comparing feature embeddings. In tasks like face recognition, models generate a feature vector (embedding) for each image. These embeddings capture the unique characteristics of the image and can be compared to determine how similar or different two images are. FaceX helps explain which parts of an image contribute to its similarity or difference to a reference embedding, allowing you to understand the model's focus on specific facial features during the comparison.</p>  <p>In this context, the key idea is that FaceX analyzes the facial regions in the image that most influence how the model compares the reference embedding with the new image's embedding. Rather than providing an explanation for individual images in isolation, FaceX aggregates information across the dataset, offering insights into how different parts of the face, such as the eyes, mouth, or hair, contribute to the similarity or difference between the reference and the image being compared.</p>  <p>FaceX works by using a "reference" image (e.g., identity image) and evaluating how other images (e.g., selfies) compare to it. It looks at how various facial regions of the new image align with the reference image in the feature space. The tool then highlights which facial regions are most influential in the comparison, showing what aspects of the image are similar to or different from the concept embedding.</p>  <p>Overall, FaceX gives you a better understanding of how the model processes and compares facial features by highlighting the specific regions that influence the similarity or difference in feature embeddings. This is especially useful for improving transparency and identifying potential biases in how face verification models represent and compare faces.</p>  <p><span class="alert alert-warning alert-dismissible fade show" role="alert" style="display: inline-block; padding: 10px;"> <i class="bi bi-exclamation-triangle-fill"></i> GPU access is recommended as this analysis can be computationally intensive, especially with large datasets. </span></p> </p>
<b>Parameters</b><br> <button
                      type="button"
                      class="btn btn-light"
                      data-bs-toggle="tooltip"
                      data-bs-placement="top"
                      title=" This variable determines what kind of comparison you want to explore. If you set the target class to 1, FaceX will investigate the regions in the image that are similar to the reference embedding (i.e., explanations on why the model consider the two images similar). If you set the target class to 0, FaceX will show the regions that are most different from the reference embedding (i.e., explanations on why the model consider the two images dissimilar)."
                      data-description=" This variable determines what kind of comparison you want to explore. If you set the target class to 1, FaceX will investigate the regions in the image that are similar to the reference embedding (i.e., explanations on why the model consider the two images similar). If you set the target class to 0, FaceX will show the regions that are most different from the reference embedding (i.e., explanations on why the model consider the two images dissimilar)."
                      data-name="Target class"
                      onclick="showDescriptionModal(this)">
                      <i class="bi bi-info-circle"></i> Target class
                    </button>
<button
                      type="button"
                      class="btn btn-light"
                      data-bs-toggle="tooltip"
                      data-bs-placement="top"
                      title=" This parameter lets you choose which part of the model's neural network you want to analyze. In simple terms, a model consists of multiple layers that process information at different levels. The target layer refers to the specific layer in the model that you want to explain. The explanation will show you which regions of the face are most important to that layer's decision-making process. For example, the deeper layers of the model may focus on more complex features like facial structure, while earlier layers might focus on simpler features like edges and textures. Typically, you should opt for the last layer producing the final embeddings."
                      data-description=" This parameter lets you choose which part of the model's neural network you want to analyze. In simple terms, a model consists of multiple layers that process information at different levels. The target layer refers to the specific layer in the model that you want to explain. The explanation will show you which regions of the face are most important to that layer's decision-making process. For example, the deeper layers of the model may focus on more complex features like facial structure, while earlier layers might focus on simpler features like edges and textures. Typically, you should opt for the last layer producing the final embeddings."
                      data-name="Target layer"
                      onclick="showDescriptionModal(this)">
                      <i class="bi bi-info-circle"></i> Target layer
                    </button>
<br>
<h2 id='connection-properties'>Connection properties</h2>
<p><p>Performs analysis of connection properties in a graph. If no sensitive attributes are provided, all node column attributes are considered sensitive.</p> </p>
<h2 id='exposure-distance-comparison'>Exposure distance comparison</h2>
<p><p>Compute the exposure distance between the protected and non-protected groups in the dataset and ranking. Parameters:</p>  <ul> <li><p><code>N runs</code>: Choose a natural number between 1 and 100</p></li> <li><p><code>Sensitive attributes</code>: Which attribute is relevant for fairness analysis.  To select this, click the blue '+' and then use the dropdown.  Currently, only <em>Gender</em> is supported</p></li> <li><p><code>Protected</code>: The protected group for the fairness analysis. Currenly, only <em>female</em> or <em>male</em> are supported</p></li> <li><p><code>Sampling Attribute</code>: The value by which we group the analysis for finer-grained results. One of <em>Nationality&#95;IncomeGroup</em> or <em>Nationality&#95;Region</em>.</p></li> <li><p><code>Ranking Variable</code>: This refers to the main criteria by which ranking is done.  One of <em>Degree</em>, <em>Citations</em> or <em>Productivity</em></p></li> </ul> </p>
<b>Parameters</b><br>N runs
Protected
Sampling attribute
Ranking variable
<br>

            </div>
        </div>
    </div>

    <!-- Description Modal -->
    <div class="modal fade" id="descriptionModal" tabindex="-1" aria-labelledby="descriptionModalLabel" aria-hidden="true">
      <div class="modal-dialog modal-lg modal-dialog-centered">
        <div class="modal-content">
          <div class="modal-header">
            <h5 class="modal-title text-muted" id="descriptionModalLabel">Description</h5>
            <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
          </div>
          <div class="modal-body" id="descriptionModalBody">
            <!-- Description will be injected here -->
          </div>
        </div>
      </div>
    </div>

<script>
    function showDescriptionModal(button) {
        const description = button.getAttribute("data-description");
        const name = button.getAttribute("data-name");
        document.getElementById("descriptionModalLabel").innerText = name;
        document.getElementById("descriptionModalBody").innerText = description;
        const modal = new bootstrap.Modal(document.getElementById("descriptionModal"));
        modal.show();
    }
</script>
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
